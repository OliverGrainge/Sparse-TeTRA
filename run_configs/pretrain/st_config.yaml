
pretrain:
  module:
    model_name: ViT
    model_init_args: 
      image_size: 224
      patch_size: 16
      dim: 768  # Increased model dimension
      depth: 12  # Increased number of layers
      heads: 12  # Increased number of attention heads 
      mlp_dim: 3072  # Increased MLP dimension
      in_channels: 3
      dim_head: 64
      dropout: 0.1
      emb_dropout: 0.1
      embedding_norm: "LayerNorm"
      embedding_linear: "Linear"
      attention_linear_layer: "SparseTernaryLinear"
      attention_norm_layer: "LayerNorm"
      feedforward_linear_layer: "SparseTernaryLinear"
      feedforward_norm_layer: "LayerNorm"
      feedforward_activation_layer: "GELU"
    
    lr: 0.0003
    weight_decay: 0.001

  data: 
    train_data_dir: "/scratch/oeg1n18/datasets/vpr/sf_xl/processed/train"
    batch_size: 384
    num_workers: 8
    img_size: 224
    pin_memory: true

  trainer:
    enable_progress_bar: false
    max_epochs: 6  # Reduced number of epochs
    accelerator: "auto"
    devices: "auto"
    precision: "16-mixed"
