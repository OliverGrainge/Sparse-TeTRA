
posttrain: 
  module: 
    model_name: ViT
    checkpoint_path: "checkpoints/checkpoints/pretrain/ViT/st_config_epoch=5-train_loss=1.9311.ckpt"

    model_init_kwargs: 
      image_size: 224
      patch_size: 16
      dim: 768  # Increased model dimension
      depth: 12  # Increased number of layers
      heads: 12  # Increased number of attention heads 
      mlp_dim: 3072  # Increased MLP dimension
      in_channels: 3
      dim_head: 64
      dropout: 0.1
      emb_dropout: 0.1
      embedding_norm: "LayerNorm"
      embedding_linear: "Linear"
      attention_linear_layer: "SparseTernaryLinear"
      attention_linear_kwargs: 
        sparsity: 0.4
      attention_norm_layer: "LayerNorm"
      feedforward_linear_layer: "SparseTernaryLinear"
      feedforward_linear_kwargs: 
        sparsity: 0.4
      feedforward_norm_layer: "LayerNorm"
      feedforward_activation_layer: "GELU"
    
    agg_name: BoQ
    agg_init_kwargs: {}


  data: 
    train_data_dir: "/home/oliver/datasets_drive/vpr_datasets/gsv-cities/" 
    val_data_dir: "/home/oliver/datasets_drive/vpr_datasets"
    batch_size: 24
    img_per_place: 4
    min_img_per_place: 4
    shuffle_all: False
    image_size:
      - 224
      - 224
    num_workers: 8
    val_set_names:
      - pitts30k

  trainer:
    max_epochs: 1 
    accelerator: "auto"
    devices: "auto"
    precision: "16-mixed"
    num_sanity_val_steps: 0
    limit_train_batches: 500
